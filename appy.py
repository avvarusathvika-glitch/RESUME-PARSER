# -*- coding: utf-8 -*-
"""appy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RGASeRX20oUoLHRTcx3zhpTnKf2iJXye
"""



# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from io import BytesIO
# from pdfminer.high_level import extract_text as pdf_extract_text
# import docx
# from PIL import Image
# import pytesseract
# import re
# import nltk
# from sentence_transformers import SentenceTransformer, util
# from transformers import pipeline
# import dateparser
# import pandas as pd
# import altair as alt
# import json
# from datetime import datetime
# from dateutil import parser as dateparser2
# import numpy as np
# 
# nltk.download('punkt', quiet=True)
# 
# st.set_page_config(layout="wide", page_title="AI Resume Parser")
# 
# # ---------- Helpers ----------
# 
# def extract_text_from_pdf_bytes(file_bytes):
#     with BytesIO(file_bytes) as f:
#         try:
#             text = pdf_extract_text(f)
#         except Exception:
#             text = ""
#     return text
# 
# def extract_text_from_docx_bytes(file_bytes):
#     doc = docx.Document(BytesIO(file_bytes))
#     full_text = [para.text for para in doc.paragraphs]
#     return "\n".join(full_text)
# 
# def extract_text_from_image_bytes(file_bytes):
#     img = Image.open(BytesIO(file_bytes)).convert("RGB")
#     text = pytesseract.image_to_string(img)
#     return text
# 
# def extract_text_smart_bytes(file_bytes, filename):
#     txt = ""
#     if filename.lower().endswith(".pdf"):
#         txt = extract_text_from_pdf_bytes(file_bytes)
#         if (not txt) or txt.strip().startswith("%PDF") or len(txt.strip()) < 80:
#             try:
#                 from pdf2image import convert_from_bytes
#                 pages = convert_from_bytes(file_bytes, dpi=200)
#                 ocr_pages = []
#                 for p in pages:
#                     ocr_pages.append(pytesseract.image_to_string(p.convert("L")))
#                 txt = "\n\n".join(ocr_pages)
#             except Exception:
#                 pass
#     elif filename.lower().endswith((".docx",".doc")):
#         txt = extract_text_from_docx_bytes(file_bytes)
#     elif filename.lower().endswith((".png",".jpg",".jpeg",".bmp",".tiff")):
#         txt = extract_text_from_image_bytes(file_bytes)
#     else:
#         try:
#             txt = file_bytes.decode('utf-8')
#         except:
#             txt = file_bytes.decode('latin-1', errors='ignore')
#     return txt
# 
# EMAIL_RE = re.compile(r'[\w\.-]+@[\w\.-]+\.\w+')
# PHONE_RE = re.compile(r'(\+?\d{1,3}[\s-]?)?(\d{10}|\d{3}[\s-]\d{3}[\s-]\d{4}|\(\d{3}\)\s*\d{3}-\d{4})')
# LINK_RE = re.compile(r'(https?://\S+|linkedin\.com/\S+|github\.com/\S+)')
# 
# CURATED_SKILLS = [
#     "python","java","c++","c","javascript","react","angular","node","django","flask",
#     "tensorflow","pytorch","keras","sql","postgresql","mysql","mongodb","docker",
#     "kubernetes","aws","azure","gcp","pandas","numpy","scikit-learn","excel","git","communication"
# ]
# 
# @st.cache_resource
# def load_models():
#     ner = pipeline("ner", model="dslim/bert-base-NER", grouped_entities=True)
#     sent_model = SentenceTransformer('all-MiniLM-L6-v2')
#     return ner, sent_model
# 
# def run_ner(ner_model, text):
#     if not text or len(text.strip()) < 10:
#         return []
#     try:
#         ents = ner_model(text)
#     except Exception:
#         ents = []
#     normalized = []
#     for e in ents:
#         label = e.get("entity_group") or e.get("entity") or e.get("label")
#         normalized.append({
#             "entity": label,
#             "word": e.get("word") or e.get("entity"),
#             "score": float(e.get("score", 0))
#         })
#     return normalized
# 
# def extractive_summary(sent_model, text, top_k=3):
#     sentences = nltk.tokenize.sent_tokenize(text)
#     if not sentences:
#         return ""
#     embeddings = sent_model.encode(sentences, convert_to_tensor=True)
#     doc_emb = embeddings.mean(dim=0)
#     cos_scores = util.cos_sim(doc_emb, embeddings)[0]
#     top_results = cos_scores.argsort(descending=True)[:top_k].tolist()
#     top_sentences = [sentences[idx] for idx in sorted(top_results)]
#     return " ".join(top_sentences)
# 
# def detect_gaps(ranges, threshold_months=6):
#     parsed = []
#     for r in ranges:
#         try:
#             s = dateparser2.parse(r['start'])
#             e = dateparser2.parse(r['end']) if r['end'].lower()!="present" else datetime.now()
#             if s and e:
#                 parsed.append((s,e,r))
#         except:
#             pass
#     parsed = sorted(parsed, key=lambda x: x[0])
#     gaps = []
#     for i in range(len(parsed)-1):
#         diff = (parsed[i+1][0].year - parsed[i][1].year)*12 + (parsed[i+1][0].month - parsed[i][1].month)
#         if diff >= threshold_months:
#             gaps.append({"gap_months": diff, "between": (parsed[i][2], parsed[i+1][2])})
#     return gaps
# 
# def group_skills(skills):
#     tech = []
#     nontech = []
#     for s in skills:
#         if s in {"python","sql","tensorflow","docker","aws","git","react","django","pandas","numpy"}:
#             tech.append(s)
#         else:
#             nontech.append(s)
#     return {"technical": tech, "non_technical": nontech}
# 
# def anonymize(parsed):
#     out = dict(parsed)
#     out['name_guess'] = "CANDIDATE_XXXX"
#     out['emails'] = ["hidden@example.com"]*len(out.get('emails',[]))
#     out['phones'] = ["hidden"]*len(out.get('phones',[]))
#     return out
# 
# def elevator_pitch(parsed):
#     top_skills = parsed.get("skills", [])[:5]
#     name = parsed.get("name_guess","Candidate")
#     if parsed.get("timeline"):
#         role = parsed['timeline'][-1].get("role") or "recent role"
#     else:
#         role = "candidate"
#     return f"{name} â€” skilled in {', '.join(top_skills)}. Recent role: {role}."
# 
# # ---------- Streamlit UI ----------
# 
# st.title("ðŸ“‘ AI Resume Parser")
# st.write("Upload resumes (PDF/DOCX/Image) â†’ Extract text â†’ Run BERT NER + spaCy rules â†’ Summarize â†’ Show insights")
# 
# uploaded = st.file_uploader("Upload resume", type=["pdf","docx","doc","png","jpg","jpeg","bmp","tiff"])
# anonymize_flag = st.checkbox("Anonymize PII", value=False)
# 
# if uploaded and st.button("Parse Resume"):
#     raw = uploaded.getvalue()
#     filename = uploaded.name
#     text = extract_text_smart_bytes(raw, filename)
#     if not text.strip():
#         st.error("Could not extract text.")
#     else:
#         st.success("Text extracted. Running models...")
# 
#         with st.expander("ðŸ“„ Raw Text"):
#             st.text_area("Extracted text", text, height=250)
# 
#         ner_model, sent_model = load_models()
#         ner_results = run_ner(ner_model, text[:30000])
#         summary = extractive_summary(sent_model, text, top_k=3)
#         skills = sorted({s for s in CURATED_SKILLS if s in text.lower()})
# 
#         emails = EMAIL_RE.findall(text)
#         phones = []
#         for m in PHONE_RE.findall(text):
#             if isinstance(m, tuple):
#                 phones.append("".join(m).strip())
#             else:
#                 phones.append(m)
#         links = LINK_RE.findall(text)
# 
#         DATE_RANGE_RE = re.compile(r'((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec|\d{1,2})[^\n,-]{0,12}\s?\d{2,4})\s*(?:to|-|â€“|â€”)\s*((?:Present|present|[A-Za-z]{3,9}\s?\d{2,4}|\d{4}))', re.IGNORECASE)
#         ranges = []
#         for m in DATE_RANGE_RE.finditer(text):
#             s_raw, e_raw = m.groups()
#             s = dateparser.parse(s_raw)
#             e = dateparser.parse(e_raw) if e_raw and e_raw.lower()!="present" else datetime.now()
#             if s:
#                 ranges.append({"start_raw":s_raw,"end_raw":e_raw,"start":s.isoformat(),"end":e.isoformat(),"role":""})
# 
#         name_guess = ""
#         for e in ner_results:
#             if e['entity'].lower()=="per":
#                 name_guess = e['word']; break
#         if not name_guess and emails:
#             name_guess = emails[0].split("@")[0]
# 
#         parsed = {
#             "filename": filename,
#             "name_guess": name_guess,
#             "emails": emails,
#             "phones": phones,
#             "links": links,
#             "skills": skills,
#             "skills_grouped": group_skills(skills),
#             "summary": summary,
#             "timeline": ranges,
#             "ner_entities": ner_results
#         }
# 
#         if anonymize_flag:
#             parsed = anonymize(parsed)
# 
#         tabs = st.tabs(["Overview","Entities","Timeline","JSON"])
# 
#         with tabs[0]:
#             st.subheader("Candidate Overview")
#             st.write("**Name:**", parsed['name_guess'])
#             st.write("**Emails:**", parsed['emails'] or "â€”")
#             st.write("**Phones:**", parsed['phones'] or "â€”")
#             st.write("**Links:**", parsed['links'] or "â€”")
#             st.write("**Skills (grouped):**", parsed['skills_grouped'])
#             st.info("**Elevator Pitch:** " + elevator_pitch(parsed))
#             st.write("**Summary:**")
#             st.success(parsed['summary'] or "â€”")
# 
#         with tabs[1]:
#             st.subheader("BERT NER Entities")
#             if parsed['ner_entities']:
#                 df = pd.DataFrame(parsed['ner_entities'])
#                 st.dataframe(df)
#             else:
#                 st.write("No entities detected.")
# 
#         with tabs[2]:
#             st.subheader("Career Timeline")
#             if parsed['timeline']:
#                 df_t = pd.DataFrame(parsed['timeline'])
#                 df_t['start_dt'] = pd.to_datetime(df_t['start'])
#                 df_t['end_dt'] = pd.to_datetime(df_t['end'])
#                 chart = alt.Chart(df_t).mark_bar().encode(
#                     x='start_dt:T',
#                     x2='end_dt:T',
#                     y=alt.Y('start_raw:N', sort=None),
#                     tooltip=['start_raw','end_raw']
#                 )
#                 st.altair_chart(chart, use_container_width=True)
#                 gaps = detect_gaps(parsed['timeline'])
#                 if gaps:
#                     st.warning(f"Detected career gaps: {gaps}")
#             else:
#                 st.write("No timeline found.")
# 
#         with tabs[3]:
#             st.subheader("JSON Export")
#             st.json(parsed)
#             st.download_button("Download JSON", data=json.dumps(parsed, indent=2), file_name="parsed_resume.json", mime="application/json")
# 
# st.caption("Built with BERT + spaCy + SentenceTransformers. ðŸš€")
#

!streamlit run app.py --server.port 8501 --server.headless true &>/dev/null&

from google.colab.output import eval_js
print("Click this link to open app ðŸ‘‰")
print(eval_js("google.colab.kernel.proxyPort(8501)"))

!tail -n 40 streamlit_log.txt

!pkill -f streamlit || true

!streamlit run app.py --server.port 8501 --server.headless true > streamlit_log.txt 2>&1 &

!tail -n 40 streamlit_log.txt

from google.colab.output import eval_js
print(eval_js("google.colab.kernel.proxyPort(8501)"))

!ps -ef | grep streamlit

!tail -n 50 streamlit_log.txt

!tail -n 100 streamlit_log.txt

from google.colab.output import eval_js
print("Click this link to open app ðŸ‘‰")
print(eval_js("google.colab.kernel.proxyPort(8501)"))

from google.colab.output import eval_js
print(eval_js("google.colab.kernel.proxyPort(8501)"))
